{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":458130,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":371373,"modelId":392277}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Restart Kernel\n!pip install -qq -U bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T05:49:29.334612Z","iopub.execute_input":"2025-07-03T05:49:29.334908Z","iopub.status.idle":"2025-07-03T05:49:32.511101Z","shell.execute_reply.started":"2025-07-03T05:49:29.334884Z","shell.execute_reply":"2025-07-03T05:49:32.509953Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport shutil\nfrom pathlib import Path\nimport os\nimport random\nimport re\nimport json\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport cv2\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom datasets import load_dataset\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nfrom torch import nn, optim\n\nfrom peft import PeftModel\nfrom transformers import (AutoModelForCausalLM, Blip2Processor, Blip2Model, BlipImageProcessor, AutoTokenizer, BitsAndBytesConfig,\n    Trainer, TrainingArguments, default_data_collator, TrainerCallback)\n\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import train_test_split\n\nfrom datasets import load_dataset\n\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\nsave_path = '/kaggle/working/'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T05:49:32.512939Z","iopub.execute_input":"2025-07-03T05:49:32.513174Z","iopub.status.idle":"2025-07-03T05:49:59.487832Z","shell.execute_reply.started":"2025-07-03T05:49:32.513153Z","shell.execute_reply":"2025-07-03T05:49:59.486985Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BLIP2ForPhi(nn.Module):\n    def __init__(self, vision_model, q_former, phi_model, query_tokens):\n        super().__init__()\n        self.vision_model = vision_model\n        self.q_former = q_former\n        self.projection = nn.Linear(q_former.config.hidden_size, phi_model.config.hidden_size)\n        self.phi_model = phi_model\n        self.query_tokens = query_tokens\n        \n\n        print(\"Freezing vision_model and phi_model parameters...\")\n        for param in self.vision_model.parameters():\n            param.requires_grad = False\n        for param in self.phi_model.parameters():\n            param.requires_grad = False\n        \n        print(\"Training q_former and projection layer...\")\n        for param in self.q_former.parameters():\n            param.requires_grad = True\n        for param in self.projection.parameters():\n            param.requires_grad = True\n\n    \n    def forward(self, pixel_values, input_ids, attention_mask, labels=None):\n        image_embeds = self.vision_model(pixel_values).last_hidden_state\n\n        batch_size = image_embeds.shape[0]\n        qformer_query_embeds = self.query_tokens.expand(batch_size, -1, -1)\n\n        \n        image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n        query_outputs = self.q_former(\n            query_embeds=qformer_query_embeds,\n            encoder_hidden_states=image_embeds,\n            encoder_attention_mask=image_attention_mask\n        )[0]\n\n        projected_query = self.projection(query_outputs)\n\n        text_embeds = self.phi_model.get_input_embeddings()(input_ids)\n        inputs_embeds = torch.cat([projected_query, text_embeds], dim=1)\n        \n        # ÏûÖÎ†• IDÏùò attention_maskÏôÄ ÏøºÎ¶¨Ïùò attention_maskÎ•º Í≤∞Ìï©\n        query_attention_mask = torch.ones(projected_query.size()[:-1], dtype=torch.long, device=projected_query.device)\n        combined_attention_mask = torch.cat([query_attention_mask, attention_mask], dim=1)\n\n        \n        outputs = self.phi_model(\n            inputs_embeds=inputs_embeds,\n            attention_mask=combined_attention_mask,\n            labels=labels, \n        )\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T05:49:59.488721Z","iopub.execute_input":"2025-07-03T05:49:59.489277Z","iopub.status.idle":"2025-07-03T05:49:59.496899Z","shell.execute_reply.started":"2025-07-03T05:49:59.489256Z","shell.execute_reply":"2025-07-03T05:49:59.496187Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"quantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\n\nimage_processor = BlipImageProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\nphi_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)\nif phi_tokenizer.pad_token is None:\n    phi_tokenizer.pad_token = phi_tokenizer.eos_token\n\nblip2_model = Blip2Model.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n\nvision_model = blip2_model.vision_model\nq_former = blip2_model.qformer\nquery_tokens = blip2_model.query_tokens\n\nphi_model = AutoModelForCausalLM.from_pretrained(\n    \"microsoft/phi-1_5\",\n    quantization_config=quantization_config,\n    trust_remote_code=True,\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-03T05:49:59.499047Z","iopub.execute_input":"2025-07-03T05:49:59.499589Z","iopub.status.idle":"2025-07-03T05:51:14.848620Z","shell.execute_reply.started":"2025-07-03T05:49:59.499560Z","shell.execute_reply":"2025-07-03T05:51:14.847997Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = BLIP2ForPhi(vision_model, q_former, phi_model, query_tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T05:51:14.849565Z","iopub.execute_input":"2025-07-03T05:51:14.849850Z","iopub.status.idle":"2025-07-03T05:51:14.881122Z","shell.execute_reply.started":"2025-07-03T05:51:14.849826Z","shell.execute_reply":"2025-07-03T05:51:14.880331Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Stage1Dataset(Dataset):\n    def __init__(self, hf_dataset, image_processor, tokenizer, num_query_tokens=32, max_length=128):\n        self.dataset = hf_dataset\n        self.image_processor = image_processor\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.num_query_tokens = num_query_tokens\n        \n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n        image = item['image'].convert(\"RGB\")\n        captions = item['answer']\n        caption = captions[torch.randint(0, len(captions), (1,)).item()].replace('\\n', ' ').strip()\n\n        \n        pixel_values = self.image_processor(image, return_tensors=\"pt\").pixel_values\n\n        inputs = self.tokenizer(\n            caption,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n        \n        text_labels = inputs.input_ids.clone()\n        text_labels[text_labels == self.tokenizer.pad_token_id] = -100\n        \n        query_labels = torch.full((1, self.num_query_tokens), -100)\n\n        combined_labels = torch.cat([query_labels, text_labels], dim=1)\n\n        return {\n            \"pixel_values\": pixel_values.squeeze(),\n            \"input_ids\": inputs.input_ids.squeeze(),   \n            \"attention_mask\": inputs.attention_mask.squeeze(),\n            \"labels\": combined_labels.squeeze()\n        }\n\nraw_dataset = load_dataset(\"lmms-lab/COCO-Caption\")['val']\n\nsplit_dataset = raw_dataset.train_test_split(test_size=0.2)\ntrain_raw_dataset = split_dataset['train']\neval_raw_dataset = split_dataset['test']\n\ntrain_dataset = Stage1Dataset(train_raw_dataset, image_processor, phi_tokenizer)\nvalid_dataset = Stage1Dataset(eval_raw_dataset, image_processor, phi_tokenizer)\ntrain_debug = Subset(train_dataset, indices=range(50))\nvalid_debug = Subset(valid_dataset, indices=range(50))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T05:51:14.881992Z","iopub.execute_input":"2025-07-03T05:51:14.882255Z","iopub.status.idle":"2025-07-03T06:02:09.317592Z","shell.execute_reply.started":"2025-07-03T05:51:14.882227Z","shell.execute_reply":"2025-07-03T06:02:09.316553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\nfrom transformers import get_cosine_schedule_with_warmup\nimport os\nfrom pathlib import Path\nfrom collections import deque\n\nclass CustomTrainer:\n    \n    def __init__(self, model: nn.Module, optimizer, tokenizer, train_dataset, val_dataset=None, batch_size=8, save_dir=\"./checkpoints\"):\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model = model.to(self.device)\n        self.optimizer = optimizer\n        self.tokenizer = tokenizer\n        \n        self.train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n        self.val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4, pin_memory=True) if val_dataset else None\n        \n        self.scaler = torch.cuda.amp.GradScaler() # ÌòºÌï© Ï†ïÎ∞ÄÎèÑ ÌïôÏäµÏö© Ïä§ÏºÄÏùºÎü¨\n        self.scheduler = None # train Î©îÏÑúÎìú ÎÇ¥ÏóêÏÑú ÏÑ§Ï†ï\n\n        self.save_dir = Path(save_dir)\n        self.save_dir.mkdir(parents=True, exist_ok=True)\n        print(f\"Using device: {self.device}\")\n\n    def _forward_step(self, batch: dict, return_preds: bool = False):\n       \n        inputs = {k: v.to(self.device) for k, v in batch.items()}\n        \n        with torch.autocast(device_type='cuda', dtype=torch.float16):\n            outputs = self.model(**inputs)\n            loss = outputs.loss\n\n        if return_preds:\n            pred_ids = torch.argmax(outputs.logits, dim=-1)\n            decoded_preds = self.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n            \n            labels = inputs['labels'].clone()\n            labels[labels == -100] = self.tokenizer.pad_token_id\n            decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n            return loss, decoded_preds, decoded_labels\n\n        return loss\n\n    def train(self, num_epochs: int, resume_from_checkpoint: str = None):\n      \n        total_steps = len(self.train_dataloader) * num_epochs\n        warmup_steps = int(0.1 * total_steps)\n        \n        self.scheduler = get_cosine_schedule_with_warmup(\n            optimizer=self.optimizer,\n            num_warmup_steps=warmup_steps,\n            num_training_steps=total_steps,\n        )\n\n        if resume_from_checkpoint:\n            start_epoch = self.load_checkpoint(resume_from_checkpoint)\n\n        for epoch in range(start_epoch, num_epochs):\n            self.model.train()\n            epoch_loss = 0\n\n            progress_bar = tqdm(self.train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\")\n            for batch in progress_bar:\n                self.optimizer.zero_grad()\n                \n                loss = self._forward_step(batch)\n                \n                self.scaler.scale(loss).backward()\n                self.scaler.step(self.optimizer)\n                self.scaler.update()\n                self.scheduler.step()\n                \n                epoch_loss += loss.item()\n                progress_bar.set_postfix(loss=loss.item(), lr=self.scheduler.get_last_lr()[0])\n            \n            avg_train_loss = epoch_loss / len(self.train_dataloader)\n            print(f\"Epoch {epoch+1} | Average Train Loss: {avg_train_loss:.4f}\")\n\n            if self.val_dataloader:\n                avg_val_loss = self.evaluate(epoch)\n            if epoch == num_epochs - 1:\n                self.save_checkpoint(epoch, avg_train_loss, avg_val_loss)\n\n    def evaluate(self, epoch: int):\n        \n        self.model.eval()\n        total_loss = 0\n        \n        last_n_samples = 5\n        last_preds = deque(maxlen=last_n_samples)\n        last_labels = deque(maxlen=last_n_samples)\n\n        with torch.no_grad():\n            progress_bar = tqdm(self.val_dataloader, desc=f\"Epoch {epoch+1} - Evaluating\")\n            for batch in progress_bar:\n                loss, decoded_preds, decoded_labels = self._forward_step(batch, return_preds=True)\n                total_loss += loss.item()\n                \n                last_preds.extend(decoded_preds)\n                last_labels.extend(decoded_labels)\n\n        avg_val_loss = total_loss / len(self.val_dataloader)\n        print(f\"\\n--- Validation Results for Epoch {epoch+1} ---\")\n        print(f\"Average Validation Loss: {avg_val_loss:.4f}\")\n        \n        print(\"\\n--- Last 5 Sample Predictions ---\")\n        for pred, label in zip(last_preds, last_labels):\n            print(f\"üîµ Pred:  {pred.strip()}\")\n            print(f\"üü¢ Label: {label.strip()}\")\n        print(\"---------------------------------------\\n\")\n\n        return avg_val_loss\n\n    def save_checkpoint(self, epoch: int, train_loss: float, val_loss: float):\n        \"\"\"Î™®Îç∏Ïùò Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Î•º Ï†ÄÏû•Ìï©ÎãàÎã§.\"\"\"\n        save_path = self.save_dir / f\"epoch_{epoch+1}\"\n        save_path.mkdir(parents=True, exist_ok=True)\n        \n        trainable_state_dict = {k: v for k, v in self.model.state_dict().items() if v.requires_grad}\n        \n        checkpoint = {\n            \"epoch\": epoch + 1,\n            \"model_state_dict\": trainable_state_dict,\n            \"optimizer_state_dict\": self.optimizer.state_dict(),\n            \"scaler_state_dict\": self.scaler.state_dict(),\n            \"scheduler_state_dict\": self.scheduler.state_dict(),\n            \"train_loss\": train_loss,\n            \"val_loss\": val_loss,\n        }\n        torch.save(checkpoint, save_path / \"checkpoint.pt\")\n        print(f\"‚úÖ Checkpoint saved to {save_path}\")\n\n    def load_checkpoint(self, checkpoint_path: str):\n       \n        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n        \n        \n        self.model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.scaler.load_state_dict(checkpoint['scaler_state_dict'])\n        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        \n        start_epoch = checkpoint['epoch']\n        print(f\"‚úÖ Checkpoint loaded. Resuming from epoch {start_epoch}\")\n        return start_epoch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T06:02:09.320370Z","iopub.execute_input":"2025-07-03T06:02:09.320723Z","iopub.status.idle":"2025-07-03T06:02:09.344628Z","shell.execute_reply.started":"2025-07-03T06:02:09.320692Z","shell.execute_reply":"2025-07-03T06:02:09.343869Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainable_params = filter(lambda p: p.requires_grad, model.parameters())\noptimizer = torch.optim.AdamW(trainable_params, lr=1e-4, weight_decay=0.01)\n\ntrainer = CustomTrainer(\n    model=model,\n    optimizer=optimizer,\n    tokenizer=phi_tokenizer,\n    train_dataset=train_dataset,\n    val_dataset=valid_dataset,\n    batch_size=4\n)\n\ncheckpoint_path = \"/kaggle/input/blip2_epoch11/pytorch/default/1/epoch_11/checkpoint.pt\"\n\ntrainer.train(num_epochs=13, resume_from_checkpoint=checkpoint_path) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T06:02:09.345560Z","iopub.execute_input":"2025-07-03T06:02:09.346033Z","execution_failed":"2025-07-03T14:58:11.029Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"shutil.make_archive(\"/kaggle/working/checkpoints\", 'zip', \"/kaggle/working/checkpoints\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-03T14:58:11.030Z"}},"outputs":[],"execution_count":null}]}