{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":39911,"sourceType":"datasetVersion","datasetId":31296},{"sourceId":459134,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":372151,"modelId":393041}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Restart Kernel\n!pip install -qq -U bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T03:41:09.019187Z","iopub.execute_input":"2025-07-05T03:41:09.019492Z","iopub.status.idle":"2025-07-05T03:41:12.160246Z","shell.execute_reply.started":"2025-07-05T03:41:09.019462Z","shell.execute_reply":"2025-07-05T03:41:12.159226Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport shutil\nfrom pathlib import Path\nimport os\nimport random\nimport re\nimport json\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport cv2\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom datasets import load_dataset\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nfrom torch import nn, optim\n\nfrom peft import PeftModel\nfrom transformers import (AutoModelForCausalLM, Blip2Processor, Blip2Model, BlipImageProcessor, AutoTokenizer, BitsAndBytesConfig,\n    Trainer, TrainingArguments, default_data_collator, TrainerCallback)\n\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import train_test_split\n\nfrom datasets import load_dataset\n\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\nsave_path = '/kaggle/working/'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T03:41:12.162026Z","iopub.execute_input":"2025-07-05T03:41:12.162276Z","iopub.status.idle":"2025-07-05T03:41:39.589303Z","shell.execute_reply.started":"2025-07-05T03:41:12.162252Z","shell.execute_reply":"2025-07-05T03:41:39.588473Z"}},"outputs":[{"name":"stderr","text":"2025-07-05 03:41:26.783991: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751686886.978708      88 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751686887.034199      88 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"class BLIP2ForPhi(nn.Module):\n    def __init__(self, vision_model, q_former, phi_model, query_tokens):\n        super().__init__()\n        self.vision_model = vision_model\n        self.q_former = q_former\n        self.projection = nn.Linear(q_former.config.hidden_size, phi_model.config.hidden_size)\n        self.phi_model = phi_model\n        self.query_tokens = query_tokens\n        \n\n        print(\"Freezing vision_model and phi_model parameters...\")\n        for param in self.vision_model.parameters():\n            param.requires_grad = False\n        for param in self.phi_model.parameters():\n            param.requires_grad = False\n        \n        print(\"Training q_former and projection layer...\")\n        for param in self.q_former.parameters():\n            param.requires_grad = True\n        for param in self.projection.parameters():\n            param.requires_grad = True\n\n    \n    def forward(self, pixel_values, input_ids, attention_mask, labels=None):\n        image_embeds = self.vision_model(pixel_values).last_hidden_state\n\n        batch_size = image_embeds.shape[0]\n        qformer_query_embeds = self.query_tokens.expand(batch_size, -1, -1)\n\n        \n        image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n        query_outputs = self.q_former(\n            query_embeds=qformer_query_embeds,\n            encoder_hidden_states=image_embeds,\n            encoder_attention_mask=image_attention_mask\n        )[0]\n\n        projected_query = self.projection(query_outputs)\n\n        text_embeds = self.phi_model.get_input_embeddings()(input_ids)\n        inputs_embeds = torch.cat([projected_query, text_embeds], dim=1)\n        \n        # ÏûÖÎ†• IDÏùò attention_maskÏôÄ ÏøºÎ¶¨Ïùò attention_maskÎ•º Í≤∞Ìï©\n        query_attention_mask = torch.ones(projected_query.size()[:-1], dtype=torch.long, device=projected_query.device)\n        combined_attention_mask = torch.cat([query_attention_mask, attention_mask], dim=1)\n\n        \n        outputs = self.phi_model(\n            inputs_embeds=inputs_embeds,\n            attention_mask=combined_attention_mask,\n            labels=labels, \n        )\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T03:41:39.590007Z","iopub.execute_input":"2025-07-05T03:41:39.590691Z","iopub.status.idle":"2025-07-05T03:41:39.598759Z","shell.execute_reply.started":"2025-07-05T03:41:39.590668Z","shell.execute_reply":"2025-07-05T03:41:39.598031Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"quantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\n\nimage_processor = BlipImageProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n\nprint(\"Mean:\", image_processor.image_mean)\nprint(\"Std:\", image_processor.image_std)\n\nphi_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)\nif phi_tokenizer.pad_token is None:\n    phi_tokenizer.pad_token = phi_tokenizer.eos_token\n\nblip2_model = Blip2Model.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n\nvision_model = blip2_model.vision_model\nq_former = blip2_model.qformer\nquery_tokens = blip2_model.query_tokens\n\nphi_model = AutoModelForCausalLM.from_pretrained(\n    \"microsoft/phi-1_5\",\n    quantization_config=quantization_config,\n    trust_remote_code=True,\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-05T03:41:39.599578Z","iopub.execute_input":"2025-07-05T03:41:39.599834Z","iopub.status.idle":"2025-07-05T03:43:03.475998Z","shell.execute_reply.started":"2025-07-05T03:41:39.599811Z","shell.execute_reply":"2025-07-05T03:43:03.475139Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81c1e5d1ed9b4af081d70ea40ec90458"}},"metadata":{}},{"name":"stdout","text":"Mean: [0.48145466, 0.4578275, 0.40821073]\nStd: [0.26862954, 0.26130258, 0.27577711]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65cef9a807a84ebb8c42c690d61dfa30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21c2ce3636f74985b4096fe12ab3644e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcdff77a23f646b98b6057bd56ff9d0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14ce4239c07d4756a82c56f0934887e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"663a6bc8f0e54741a09e91227eb3b321"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"067e314c5be94dba89a66d54f7b1dfd1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20a2c0dc0f3b45159e6cc9064d58d080"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87ffeabaa4e1463a9f3ebbaa87c7f096"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"521f942425454b9f8398707ef743b4ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b84ebc0cd8094fac97beb32604723865"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/10.0G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b21c7544d7cb4158864f710241d344d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f9716ff3fbe4b19b2362989d8825cfb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/736 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d57c4089acf4355a2405b2c61dceda9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.84G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6beab91533d94b39bfae121a58338acd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c4cb2af163c4d4180eed0b2c9f27c80"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"model = BLIP2ForPhi(vision_model, q_former, phi_model, query_tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T03:43:03.477550Z","iopub.execute_input":"2025-07-05T03:43:03.477803Z","iopub.status.idle":"2025-07-05T03:43:03.517952Z","shell.execute_reply.started":"2025-07-05T03:43:03.477776Z","shell.execute_reply":"2025-07-05T03:43:03.517074Z"}},"outputs":[{"name":"stdout","text":"Freezing vision_model and phi_model parameters...\nTraining q_former and projection layer...\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"class Stage1Dataset(Dataset):\n    def __init__(self, dataframe, image_processor, tokenizer, num_query_tokens=32, max_length=128, is_train=True):\n        self.dataset = dataframe\n        self.image_processor = image_processor\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.num_query_tokens = num_query_tokens\n        self.image_dir = '/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/'\n        self.is_train = is_train\n\n        if self.is_train:\n            self.transforms = transforms.Compose([\n                transforms.RandomResizedCrop(224, scale=(0.5, 1.0)),\n                transforms.RandomRotation(degrees=15), # -15ÎèÑÏóêÏÑú +15ÎèÑ ÏÇ¨Ïù¥Î°ú ÎûúÎç§ÌïòÍ≤å ÌöåÏ†Ñ\n                transforms.RandomHorizontalFlip(p=0.4),\n                transforms.RandomVerticalFlip(p=0.4),\n                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n                transforms.ToTensor(), \n                transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]) # Ï†ïÍ∑úÌôî\n            ])\n        else: \n            self.transforms = transforms.Compose([\n                transforms.Resize((224, 224)),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n            ])\n        \n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        row = self.dataset.iloc[idx]\n        \n        image_name = row['image_name']\n        captions = row[' comment'] # Ï∫°ÏÖò Î¶¨Ïä§Ìä∏\n        caption = random.choice(captions)\n        \n        image_path = os.path.join(self.image_dir, image_name)\n\n        try:\n            image = Image.open(image_path).convert(\"RGB\")\n        except FileNotFoundError:\n            print(f\"Warning: Image file not found at {image_path}. Skipping.\")\n            return self.__getitem__(random.randint(0, len(self) - 1))\n\n        \n        # pixel_values = self.image_processor(image, return_tensors=\"pt\").pixel_values\n        pixel_values = self.transforms(image)\n\n        inputs = self.tokenizer(\n            caption,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n        \n        text_labels = inputs.input_ids.clone()\n        text_labels[text_labels == self.tokenizer.pad_token_id] = -100\n        \n        query_labels = torch.full((1, self.num_query_tokens), -100)\n\n        combined_labels = torch.cat([query_labels, text_labels], dim=1)\n\n        return {\n            \"pixel_values\": pixel_values.squeeze(),\n            \"input_ids\": inputs.input_ids.squeeze(),   \n            \"attention_mask\": inputs.attention_mask.squeeze(),\n            \"labels\": combined_labels.squeeze()\n        }\n\ntrain_captions = pd.read_csv('/kaggle/input/flickr-image-dataset/flickr30k_images/results.csv', delimiter='|')\ntrain_captions = train_captions.dropna(subset=[' comment', 'image_name'])\ntrain_captions = train_captions.groupby('image_name')[' comment'].agg(list).reset_index()\n\ntrain_df, eval_df = train_test_split(\n    train_captions,      \n    test_size=0.2,   \n    random_state=42  \n)\n\ntrain_dataset = Stage1Dataset(train_df, image_processor, phi_tokenizer)\nvalid_dataset = Stage1Dataset(eval_df, image_processor, phi_tokenizer)\ntrain_debug = Subset(train_dataset, indices=range(50))\nvalid_debug = Subset(valid_dataset, indices=range(50))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T03:43:03.518914Z","iopub.execute_input":"2025-07-05T03:43:03.519229Z","iopub.status.idle":"2025-07-05T03:43:14.972367Z","shell.execute_reply.started":"2025-07-05T03:43:03.519199Z","shell.execute_reply":"2025-07-05T03:43:14.971516Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\nfrom transformers import get_cosine_schedule_with_warmup\nimport os\nfrom pathlib import Path\nfrom collections import deque\n\nclass CustomTrainer:\n    \"\"\"\n    ÏàòÎèôÏúºÎ°ú ÌïôÏäµ Î∞è ÌèâÍ∞Ä Î£®ÌîÑÎ•º Ï†úÏñ¥ÌïòÍ∏∞ ÏúÑÌïú Ïª§Ïä§ÌÖÄ Ìä∏Î†àÏù¥ÎÑà ÌÅ¥ÎûòÏä§ÏûÖÎãàÎã§.\n    (Îã®Í≥ÑÎ≥Ñ ÌïôÏäµ Î°úÏßÅÏù¥ Ï†úÍ±∞Îêú Í∞ÑÏÜåÌôî Î≤ÑÏ†Ñ)\n    \"\"\"\n    def __init__(self, model: nn.Module, optimizer, tokenizer, train_dataset, val_dataset=None, batch_size=8, save_dir=\"./checkpoints\"):\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model = model.to(self.device)\n        self.optimizer = optimizer\n        self.tokenizer = tokenizer\n        \n        self.train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n        self.val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4, pin_memory=True) if val_dataset else None\n        \n        self.scaler = torch.cuda.amp.GradScaler() # ÌòºÌï© Ï†ïÎ∞ÄÎèÑ ÌïôÏäµÏö© Ïä§ÏºÄÏùºÎü¨\n        self.scheduler = None # train Î©îÏÑúÎìú ÎÇ¥ÏóêÏÑú ÏÑ§Ï†ï\n\n        self.save_dir = Path(save_dir)\n        self.save_dir.mkdir(parents=True, exist_ok=True)\n        print(f\"Using device: {self.device}\")\n\n    def _forward_step(self, batch: dict, return_preds: bool = False):\n        \"\"\"TrainÍ≥º EvalÏóêÏÑú Ï§ëÎ≥µÎêòÎäî Î™®Îç∏ Ïã§Ìñâ Î∞è ÏÜêÏã§ Í≥ÑÏÇ∞ Î°úÏßÅÏùÑ ÌÜµÌï©Ìï©ÎãàÎã§.\"\"\"\n        inputs = {k: v.to(self.device) for k, v in batch.items()}\n        \n        with torch.autocast(device_type='cuda', dtype=torch.float16):\n            outputs = self.model(**inputs)\n            loss = outputs.loss\n\n        if return_preds:\n            pred_ids = torch.argmax(outputs.logits, dim=-1)\n            decoded_preds = self.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n            \n            labels = inputs['labels'].clone()\n            labels[labels == -100] = self.tokenizer.pad_token_id\n            decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n            return loss, decoded_preds, decoded_labels\n\n        return loss\n\n    def train(self, num_epochs: int, resume_from_checkpoint: str = None):\n        \"\"\"ÏßÄÏ†ïÎêú ÏóêÌè¨ÌÅ¨ ÏàòÎßåÌÅº Î™®Îç∏ÏùÑ ÌïôÏäµÌï©ÎãàÎã§.\"\"\"\n        total_steps = len(self.train_dataloader) * num_epochs\n        warmup_steps = int(0.1 * total_steps)\n        \n        self.scheduler = get_cosine_schedule_with_warmup(\n            optimizer=self.optimizer,\n            num_warmup_steps=warmup_steps,\n            num_training_steps=total_steps,\n        )\n\n        if resume_from_checkpoint:\n            start_epoch = self.load_checkpoint(resume_from_checkpoint)\n\n        for epoch in range(start_epoch, num_epochs):\n            self.model.train()\n            epoch_loss = 0\n\n            progress_bar = tqdm(self.train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\")\n            for batch in progress_bar:\n                self.optimizer.zero_grad()\n                \n                loss = self._forward_step(batch)\n                \n                self.scaler.scale(loss).backward()\n                self.scaler.step(self.optimizer)\n                self.scaler.update()\n                self.scheduler.step()\n                \n                epoch_loss += loss.item()\n                progress_bar.set_postfix(loss=loss.item(), lr=self.scheduler.get_last_lr()[0])\n            \n            avg_train_loss = epoch_loss / len(self.train_dataloader)\n            print(f\"Epoch {epoch+1} | Average Train Loss: {avg_train_loss:.4f}\")\n\n            if self.val_dataloader:\n                avg_val_loss = self.evaluate(epoch)\n            if epoch == num_epochs - 1:\n                self.save_checkpoint(epoch, avg_train_loss, avg_val_loss)\n\n    def evaluate(self, epoch: int):\n        \"\"\"Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞ÏÖãÏúºÎ°ú Î™®Îç∏ ÏÑ±Îä•ÏùÑ ÌèâÍ∞ÄÌï©ÎãàÎã§.\"\"\"\n        self.model.eval()\n        total_loss = 0\n        \n        last_n_samples = 5\n        last_preds = deque(maxlen=last_n_samples)\n        last_labels = deque(maxlen=last_n_samples)\n\n        with torch.no_grad():\n            progress_bar = tqdm(self.val_dataloader, desc=f\"Epoch {epoch+1} - Evaluating\")\n            for batch in progress_bar:\n                loss, decoded_preds, decoded_labels = self._forward_step(batch, return_preds=True)\n                total_loss += loss.item()\n                \n                last_preds.extend(decoded_preds)\n                last_labels.extend(decoded_labels)\n\n        avg_val_loss = total_loss / len(self.val_dataloader)\n        print(f\"\\n--- Validation Results for Epoch {epoch+1} ---\")\n        print(f\"Average Validation Loss: {avg_val_loss:.4f}\")\n        \n        print(\"\\n--- Last 5 Sample Predictions ---\")\n        for pred, label in zip(last_preds, last_labels):\n            print(f\"üîµ Pred:  {pred.strip()}\")\n            print(f\"üü¢ Label: {label.strip()}\")\n        print(\"---------------------------------------\\n\")\n\n        return avg_val_loss\n\n    def save_checkpoint(self, epoch: int, train_loss: float, val_loss: float):\n        \"\"\"Î™®Îç∏Ïùò Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Î•º Ï†ÄÏû•Ìï©ÎãàÎã§.\"\"\"\n        save_path = self.save_dir / f\"epoch_{epoch+1}\"\n        save_path.mkdir(parents=True, exist_ok=True)\n        \n        trainable_state_dict = {k: v for k, v in self.model.state_dict().items() if v.requires_grad}\n        \n        checkpoint = {\n            \"epoch\": epoch + 1,\n            \"model_state_dict\": trainable_state_dict,\n            \"optimizer_state_dict\": self.optimizer.state_dict(),\n            \"scaler_state_dict\": self.scaler.state_dict(),\n            \"scheduler_state_dict\": self.scheduler.state_dict(),\n            \"train_loss\": train_loss,\n            \"val_loss\": val_loss,\n        }\n        torch.save(checkpoint, save_path / \"checkpoint.pt\")\n        print(f\"‚úÖ Checkpoint saved to {save_path}\")\n\n    def load_checkpoint(self, checkpoint_path: str):\n        \"\"\"Ï†ÄÏû•Îêú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ ÌååÏùºÏùÑ Î∂àÎü¨ÏôÄ ÌïôÏäµ ÏÉÅÌÉúÎ•º Î≥µÏõêÌï©ÎãàÎã§.\"\"\"\n        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n        \n        # strict=False: Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Ïóê Ï†ÄÏû•Îêú ÌååÎùºÎØ∏ÌÑ∞Îßå Î∂àÎü¨Ïò§Í≥†, ÏóÜÎäî ÌååÎùºÎØ∏ÌÑ∞Îäî Î¨¥Ïãú\n        self.model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.scaler.load_state_dict(checkpoint['scaler_state_dict'])\n        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        \n        start_epoch = checkpoint['epoch']\n        print(f\"‚úÖ Checkpoint loaded. Resuming from epoch {start_epoch}\")\n        return start_epoch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T03:43:14.973308Z","iopub.execute_input":"2025-07-05T03:43:14.973545Z","iopub.status.idle":"2025-07-05T03:43:14.990851Z","shell.execute_reply.started":"2025-07-05T03:43:14.973525Z","shell.execute_reply":"2025-07-05T03:43:14.990130Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"trainable_params = filter(lambda p: p.requires_grad, model.parameters())\noptimizer = torch.optim.AdamW(trainable_params, lr=1e-5, weight_decay=0.01)\n\n# 3. CustomTrainer Ïù∏Ïä§ÌÑ¥Ïä§Ìôî\n# ÏÉùÏÑ±ÏûêÏóê optimizerÎ•º Ï†ÑÎã¨Ìï©ÎãàÎã§.\ntrainer = CustomTrainer(\n    model=model,\n    optimizer=optimizer,\n    tokenizer=phi_tokenizer,\n    train_dataset=train_dataset,\n    val_dataset=valid_dataset,\n    batch_size=4\n)\n\ncheckpoint_path = \"/kaggle/input/blip2_epoch14/pytorch/default/1/epoch_14/checkpoint.pt\"\n\n# 4. ÌïôÏäµ ÏãúÏûë\ntrainer.train(num_epochs=18, resume_from_checkpoint=checkpoint_path) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T03:43:14.991636Z","iopub.execute_input":"2025-07-05T03:43:14.991867Z","iopub.status.idle":"2025-07-05T03:44:32.894460Z","shell.execute_reply.started":"2025-07-05T03:43:14.991851Z","shell.execute_reply":"2025-07-05T03:44:32.893176Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_88/2001585076.py:24: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler() # ÌòºÌï© Ï†ïÎ∞ÄÎèÑ ÌïôÏäµÏö© Ïä§ÏºÄÏùºÎü¨\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n‚úÖ Checkpoint loaded. Resuming from epoch 12\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 13/14 - Training:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebe749cc37e548d6b0bcc509420cf66c"}},"metadata":{}},{"name":"stdout","text":"Epoch 13 | Average Train Loss: 4.7302\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 13 - Evaluating:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cabf051621044d4a9772c66e9a892eef"}},"metadata":{}},{"name":"stdout","text":"\n--- Validation Results for Epoch 13 ---\nAverage Validation Loss: 3.5312\n\n--- Last 5 Sample Predictions ---\nüîµ Pred:  .,, a a\n a with\n with with with girl\nAG\n with with with\n with a\n with.\n\n\n AA. A sun throws holding a baseball and with and\nüü¢ Label: The pitcher is wearing a red uniform shirt.\nüîµ Pred:  ,,,,, with with with\n with with with\n with\n\n with with with with with with with with with a a a a a A A in walking or sitting in for line line car\nüü¢ Label: People either standing or sitting waiting in a subway.\nüîµ Pred:  ,,, a\n\n a\n D D with\n\nD D\n with with with\nANDANDAND standing WITH a..\n\n. A man and through the street. catch for the..\nüü¢ Label: Young men run on the beach to train for football.\nüîµ Pred:  ,,, a,\n\n\nD with with\n\n. D A with with with\n with.. with. a\n aMAN\n A A man in standingboarding in the lake.\nüü¢ Label: a man is wakeboarding in a lake\nüîµ Pred:  .,,,\n\n with with\n with with\n\n\nA's with with with\n\nAÔøΩ.. a\n\nED\n A A man man player is throwing a bat bat.\nüü¢ Label: A male baseball player is holding a baseball.\n---------------------------------------\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 14/14 - Training:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3cdb3818c4e40ea8e5f814c3557d4f8"}},"metadata":{}},{"name":"stdout","text":"Epoch 14 | Average Train Loss: 3.4292\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 14 - Evaluating:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba10e8d2463744c9b18b940ce77831bb"}},"metadata":{}},{"name":"stdout","text":"\n--- Validation Results for Epoch 14 ---\nAverage Validation Loss: 4.2926\n\n--- Last 5 Sample Predictions ---\nüîµ Pred:  .............. man.. man. man ) ) ) ) ) )\n man man ) ) A man player in the uniform uniform. a ball to a court...\nüü¢ Label: A baseball player in a red jersey throwing a ball at the pitchers mound.\nüîµ Pred:  ....... man man man. ).. man. man man ) ) ) ) ) ) ) ) ) ) ) ) ) A in walking on the street.. for the train to\nüü¢ Label: People are standing on a train platform waiting for the train.\nüîµ Pred:  ....... man ).. ).. man.. ). ) ) ) ) ) ) ) ) ) ) ) ) A man of people children in playing towards. the track.\nüü¢ Label: A group of young men are running together on a beach.\nüîµ Pred:  ...... ) man...... man man. ). ) ) ) ). ) ) ) ) man ) ) A man inhes flying in a sense of\nüü¢ Label: A man kite surfing creates a wave.\nüîµ Pred:  .............. man...... ) ) ) ) ). ). ) ) A man in a uniform background glove standing a man background. standing a ball. the park field.\nüü¢ Label: A man in a gray baseball uniform and a black hat is catching a ball in a baseball mit.\n---------------------------------------\n\n‚úÖ Checkpoint saved to checkpoints/epoch_14\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"shutil.make_archive(\"/kaggle/working/checkpoints\", 'zip', \"/kaggle/working/checkpoints\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T03:44:32.897387Z","iopub.execute_input":"2025-07-05T03:44:32.897731Z","execution_failed":"2025-07-05T03:45:06.959Z"}},"outputs":[],"execution_count":null}]}