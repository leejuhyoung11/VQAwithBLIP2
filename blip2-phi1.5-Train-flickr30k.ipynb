{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":39911,"sourceType":"datasetVersion","datasetId":31296},{"sourceId":459134,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":372151,"modelId":393041}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Restart Kernel\n!pip install -qq -U bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T03:41:09.019187Z","iopub.execute_input":"2025-07-05T03:41:09.019492Z","iopub.status.idle":"2025-07-05T03:41:12.160246Z","shell.execute_reply.started":"2025-07-05T03:41:09.019462Z","shell.execute_reply":"2025-07-05T03:41:12.159226Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport shutil\nfrom pathlib import Path\nimport os\nimport random\nimport re\nimport json\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport cv2\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom datasets import load_dataset\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nfrom torch import nn, optim\n\nfrom peft import PeftModel\nfrom transformers import (AutoModelForCausalLM, Blip2Processor, Blip2Model, BlipImageProcessor, AutoTokenizer, BitsAndBytesConfig,\n    Trainer, TrainingArguments, default_data_collator, TrainerCallback)\n\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import train_test_split\n\nfrom datasets import load_dataset\n\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\nsave_path = '/kaggle/working/'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T03:41:12.162026Z","iopub.execute_input":"2025-07-05T03:41:12.162276Z","iopub.status.idle":"2025-07-05T03:41:39.589303Z","shell.execute_reply.started":"2025-07-05T03:41:12.162252Z","shell.execute_reply":"2025-07-05T03:41:39.588473Z"}},"outputs":[{"name":"stderr","text":"2025-07-05 03:41:26.783991: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751686886.978708      88 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751686887.034199      88 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"class BLIP2ForPhi(nn.Module):\n    def __init__(self, vision_model, q_former, phi_model, query_tokens):\n        super().__init__()\n        self.vision_model = vision_model\n        self.q_former = q_former\n        self.projection = nn.Linear(q_former.config.hidden_size, phi_model.config.hidden_size)\n        self.phi_model = phi_model\n        self.query_tokens = query_tokens\n        \n\n        print(\"Freezing vision_model and phi_model parameters...\")\n        for param in self.vision_model.parameters():\n            param.requires_grad = False\n        for param in self.phi_model.parameters():\n            param.requires_grad = False\n        \n        print(\"Training q_former and projection layer...\")\n        for param in self.q_former.parameters():\n            param.requires_grad = True\n        for param in self.projection.parameters():\n            param.requires_grad = True\n\n    \n    def forward(self, pixel_values, input_ids, attention_mask, labels=None):\n        image_embeds = self.vision_model(pixel_values).last_hidden_state\n\n        batch_size = image_embeds.shape[0]\n        qformer_query_embeds = self.query_tokens.expand(batch_size, -1, -1)\n\n        \n        image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n        query_outputs = self.q_former(\n            query_embeds=qformer_query_embeds,\n            encoder_hidden_states=image_embeds,\n            encoder_attention_mask=image_attention_mask\n        )[0]\n\n        projected_query = self.projection(query_outputs)\n\n        text_embeds = self.phi_model.get_input_embeddings()(input_ids)\n        inputs_embeds = torch.cat([projected_query, text_embeds], dim=1)\n        \n        # 입력 ID의 attention_mask와 쿼리의 attention_mask를 결합\n        query_attention_mask = torch.ones(projected_query.size()[:-1], dtype=torch.long, device=projected_query.device)\n        combined_attention_mask = torch.cat([query_attention_mask, attention_mask], dim=1)\n\n        \n        outputs = self.phi_model(\n            inputs_embeds=inputs_embeds,\n            attention_mask=combined_attention_mask,\n            labels=labels, \n        )\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T03:41:39.590007Z","iopub.execute_input":"2025-07-05T03:41:39.590691Z","iopub.status.idle":"2025-07-05T03:41:39.598759Z","shell.execute_reply.started":"2025-07-05T03:41:39.590668Z","shell.execute_reply":"2025-07-05T03:41:39.598031Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"quantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\n\nimage_processor = BlipImageProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n\nprint(\"Mean:\", image_processor.image_mean)\nprint(\"Std:\", image_processor.image_std)\n\nphi_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)\nif phi_tokenizer.pad_token is None:\n    phi_tokenizer.pad_token = phi_tokenizer.eos_token\n\nblip2_model = Blip2Model.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n\nvision_model = blip2_model.vision_model\nq_former = blip2_model.qformer\nquery_tokens = blip2_model.query_tokens\n\nphi_model = AutoModelForCausalLM.from_pretrained(\n    \"microsoft/phi-1_5\",\n    quantization_config=quantization_config,\n    trust_remote_code=True,\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-05T03:41:39.599578Z","iopub.execute_input":"2025-07-05T03:41:39.599834Z","iopub.status.idle":"2025-07-05T03:43:03.475998Z","shell.execute_reply.started":"2025-07-05T03:41:39.599811Z","shell.execute_reply":"2025-07-05T03:43:03.475139Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81c1e5d1ed9b4af081d70ea40ec90458"}},"metadata":{}},{"name":"stdout","text":"Mean: [0.48145466, 0.4578275, 0.40821073]\nStd: [0.26862954, 0.26130258, 0.27577711]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65cef9a807a84ebb8c42c690d61dfa30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21c2ce3636f74985b4096fe12ab3644e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcdff77a23f646b98b6057bd56ff9d0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14ce4239c07d4756a82c56f0934887e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"663a6bc8f0e54741a09e91227eb3b321"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"067e314c5be94dba89a66d54f7b1dfd1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20a2c0dc0f3b45159e6cc9064d58d080"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87ffeabaa4e1463a9f3ebbaa87c7f096"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"521f942425454b9f8398707ef743b4ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b84ebc0cd8094fac97beb32604723865"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/10.0G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b21c7544d7cb4158864f710241d344d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f9716ff3fbe4b19b2362989d8825cfb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/736 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d57c4089acf4355a2405b2c61dceda9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.84G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6beab91533d94b39bfae121a58338acd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c4cb2af163c4d4180eed0b2c9f27c80"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"model = BLIP2ForPhi(vision_model, q_former, phi_model, query_tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T03:43:03.477550Z","iopub.execute_input":"2025-07-05T03:43:03.477803Z","iopub.status.idle":"2025-07-05T03:43:03.517952Z","shell.execute_reply.started":"2025-07-05T03:43:03.477776Z","shell.execute_reply":"2025-07-05T03:43:03.517074Z"}},"outputs":[{"name":"stdout","text":"Freezing vision_model and phi_model parameters...\nTraining q_former and projection layer...\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"class Stage1Dataset(Dataset):\n    def __init__(self, dataframe, image_processor, tokenizer, num_query_tokens=32, max_length=128, is_train=True):\n        self.dataset = dataframe\n        self.image_processor = image_processor\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.num_query_tokens = num_query_tokens\n        self.image_dir = '/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/'\n        self.is_train = is_train\n\n        if self.is_train:\n            self.transforms = transforms.Compose([\n                transforms.RandomResizedCrop(224, scale=(0.5, 1.0)),\n                transforms.RandomRotation(degrees=15), # -15도에서 +15도 사이로 랜덤하게 회전\n                transforms.RandomHorizontalFlip(p=0.4),\n                transforms.RandomVerticalFlip(p=0.4),\n                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n                transforms.ToTensor(), \n                transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]) # 정규화\n            ])\n        else: \n            self.transforms = transforms.Compose([\n                transforms.Resize((224, 224)),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n            ])\n        \n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        row = self.dataset.iloc[idx]\n        \n        image_name = row['image_name']\n        captions = row[' comment'] # 캡션 리스트\n        caption = random.choice(captions)\n        \n        image_path = os.path.join(self.image_dir, image_name)\n\n        try:\n            image = Image.open(image_path).convert(\"RGB\")\n        except FileNotFoundError:\n            print(f\"Warning: Image file not found at {image_path}. Skipping.\")\n            return self.__getitem__(random.randint(0, len(self) - 1))\n\n        \n        # pixel_values = self.image_processor(image, return_tensors=\"pt\").pixel_values\n        pixel_values = self.transforms(image)\n\n        inputs = self.tokenizer(\n            caption,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n        \n        text_labels = inputs.input_ids.clone()\n        text_labels[text_labels == self.tokenizer.pad_token_id] = -100\n        \n        query_labels = torch.full((1, self.num_query_tokens), -100)\n\n        combined_labels = torch.cat([query_labels, text_labels], dim=1)\n\n        return {\n            \"pixel_values\": pixel_values.squeeze(),\n            \"input_ids\": inputs.input_ids.squeeze(),   \n            \"attention_mask\": inputs.attention_mask.squeeze(),\n            \"labels\": combined_labels.squeeze()\n        }\n\ntrain_captions = pd.read_csv('/kaggle/input/flickr-image-dataset/flickr30k_images/results.csv', delimiter='|')\ntrain_captions = train_captions.dropna(subset=[' comment', 'image_name'])\ntrain_captions = train_captions.groupby('image_name')[' comment'].agg(list).reset_index()\n\ntrain_df, eval_df = train_test_split(\n    train_captions,      \n    test_size=0.2,   \n    random_state=42  \n)\n\ntrain_dataset = Stage1Dataset(train_df, image_processor, phi_tokenizer)\nvalid_dataset = Stage1Dataset(eval_df, image_processor, phi_tokenizer)\ntrain_debug = Subset(train_dataset, indices=range(50))\nvalid_debug = Subset(valid_dataset, indices=range(50))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T03:43:03.518914Z","iopub.execute_input":"2025-07-05T03:43:03.519229Z","iopub.status.idle":"2025-07-05T03:43:14.972367Z","shell.execute_reply.started":"2025-07-05T03:43:03.519199Z","shell.execute_reply":"2025-07-05T03:43:14.971516Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\nfrom transformers import get_cosine_schedule_with_warmup\nimport os\nfrom pathlib import Path\nfrom collections import deque\n\nclass CustomTrainer:\n    \"\"\"\n    수동으로 학습 및 평가 루프를 제어하기 위한 커스텀 트레이너 클래스입니다.\n    (단계별 학습 로직이 제거된 간소화 버전)\n    \"\"\"\n    def __init__(self, model: nn.Module, optimizer, tokenizer, train_dataset, val_dataset=None, batch_size=8, save_dir=\"./checkpoints\"):\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model = model.to(self.device)\n        self.optimizer = optimizer\n        self.tokenizer = tokenizer\n        \n        self.train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n        self.val_dataloader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4, pin_memory=True) if val_dataset else None\n        \n        self.scaler = torch.cuda.amp.GradScaler() # 혼합 정밀도 학습용 스케일러\n        self.scheduler = None # train 메서드 내에서 설정\n\n        self.save_dir = Path(save_dir)\n        self.save_dir.mkdir(parents=True, exist_ok=True)\n        print(f\"Using device: {self.device}\")\n\n    def _forward_step(self, batch: dict, return_preds: bool = False):\n        \"\"\"Train과 Eval에서 중복되는 모델 실행 및 손실 계산 로직을 통합합니다.\"\"\"\n        inputs = {k: v.to(self.device) for k, v in batch.items()}\n        \n        with torch.autocast(device_type='cuda', dtype=torch.float16):\n            outputs = self.model(**inputs)\n            loss = outputs.loss\n\n        if return_preds:\n            pred_ids = torch.argmax(outputs.logits, dim=-1)\n            decoded_preds = self.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n            \n            labels = inputs['labels'].clone()\n            labels[labels == -100] = self.tokenizer.pad_token_id\n            decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n            return loss, decoded_preds, decoded_labels\n\n        return loss\n\n    def train(self, num_epochs: int, resume_from_checkpoint: str = None):\n        \"\"\"지정된 에포크 수만큼 모델을 학습합니다.\"\"\"\n        total_steps = len(self.train_dataloader) * num_epochs\n        warmup_steps = int(0.1 * total_steps)\n        \n        self.scheduler = get_cosine_schedule_with_warmup(\n            optimizer=self.optimizer,\n            num_warmup_steps=warmup_steps,\n            num_training_steps=total_steps,\n        )\n\n        if resume_from_checkpoint:\n            start_epoch = self.load_checkpoint(resume_from_checkpoint)\n\n        for epoch in range(start_epoch, num_epochs):\n            self.model.train()\n            epoch_loss = 0\n\n            progress_bar = tqdm(self.train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\")\n            for batch in progress_bar:\n                self.optimizer.zero_grad()\n                \n                loss = self._forward_step(batch)\n                \n                self.scaler.scale(loss).backward()\n                self.scaler.step(self.optimizer)\n                self.scaler.update()\n                self.scheduler.step()\n                \n                epoch_loss += loss.item()\n                progress_bar.set_postfix(loss=loss.item(), lr=self.scheduler.get_last_lr()[0])\n            \n            avg_train_loss = epoch_loss / len(self.train_dataloader)\n            print(f\"Epoch {epoch+1} | Average Train Loss: {avg_train_loss:.4f}\")\n\n            if self.val_dataloader:\n                avg_val_loss = self.evaluate(epoch)\n            if epoch == num_epochs - 1:\n                self.save_checkpoint(epoch, avg_train_loss, avg_val_loss)\n\n    def evaluate(self, epoch: int):\n        \"\"\"검증 데이터셋으로 모델 성능을 평가합니다.\"\"\"\n        self.model.eval()\n        total_loss = 0\n        \n        last_n_samples = 5\n        last_preds = deque(maxlen=last_n_samples)\n        last_labels = deque(maxlen=last_n_samples)\n\n        with torch.no_grad():\n            progress_bar = tqdm(self.val_dataloader, desc=f\"Epoch {epoch+1} - Evaluating\")\n            for batch in progress_bar:\n                loss, decoded_preds, decoded_labels = self._forward_step(batch, return_preds=True)\n                total_loss += loss.item()\n                \n                last_preds.extend(decoded_preds)\n                last_labels.extend(decoded_labels)\n\n        avg_val_loss = total_loss / len(self.val_dataloader)\n        print(f\"\\n--- Validation Results for Epoch {epoch+1} ---\")\n        print(f\"Average Validation Loss: {avg_val_loss:.4f}\")\n        \n        print(\"\\n--- Last 5 Sample Predictions ---\")\n        for pred, label in zip(last_preds, last_labels):\n            print(f\"🔵 Pred:  {pred.strip()}\")\n            print(f\"🟢 Label: {label.strip()}\")\n        print(\"---------------------------------------\\n\")\n\n        return avg_val_loss\n\n    def save_checkpoint(self, epoch: int, train_loss: float, val_loss: float):\n        \"\"\"모델의 체크포인트를 저장합니다.\"\"\"\n        save_path = self.save_dir / f\"epoch_{epoch+1}\"\n        save_path.mkdir(parents=True, exist_ok=True)\n        \n        trainable_state_dict = {k: v for k, v in self.model.state_dict().items() if v.requires_grad}\n        \n        checkpoint = {\n            \"epoch\": epoch + 1,\n            \"model_state_dict\": trainable_state_dict,\n            \"optimizer_state_dict\": self.optimizer.state_dict(),\n            \"scaler_state_dict\": self.scaler.state_dict(),\n            \"scheduler_state_dict\": self.scheduler.state_dict(),\n            \"train_loss\": train_loss,\n            \"val_loss\": val_loss,\n        }\n        torch.save(checkpoint, save_path / \"checkpoint.pt\")\n        print(f\"✅ Checkpoint saved to {save_path}\")\n\n    def load_checkpoint(self, checkpoint_path: str):\n        \"\"\"저장된 체크포인트 파일을 불러와 학습 상태를 복원합니다.\"\"\"\n        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n        \n        # strict=False: 체크포인트에 저장된 파라미터만 불러오고, 없는 파라미터는 무시\n        self.model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.scaler.load_state_dict(checkpoint['scaler_state_dict'])\n        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        \n        start_epoch = checkpoint['epoch']\n        print(f\"✅ Checkpoint loaded. Resuming from epoch {start_epoch}\")\n        return start_epoch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T03:43:14.973308Z","iopub.execute_input":"2025-07-05T03:43:14.973545Z","iopub.status.idle":"2025-07-05T03:43:14.990851Z","shell.execute_reply.started":"2025-07-05T03:43:14.973525Z","shell.execute_reply":"2025-07-05T03:43:14.990130Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"trainable_params = filter(lambda p: p.requires_grad, model.parameters())\noptimizer = torch.optim.AdamW(trainable_params, lr=1e-5, weight_decay=0.01)\n\n# 3. CustomTrainer 인스턴스화\n# 생성자에 optimizer를 전달합니다.\ntrainer = CustomTrainer(\n    model=model,\n    optimizer=optimizer,\n    tokenizer=phi_tokenizer,\n    train_dataset=train_dataset,\n    val_dataset=valid_dataset,\n    batch_size=4\n)\n\ncheckpoint_path = \"/kaggle/input/blip2_epoch14/pytorch/default/1/epoch_14/checkpoint.pt\"\n\n# 4. 학습 시작\ntrainer.train(num_epochs=18, resume_from_checkpoint=checkpoint_path) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T03:43:14.991636Z","iopub.execute_input":"2025-07-05T03:43:14.991867Z","iopub.status.idle":"2025-07-05T03:44:32.894460Z","shell.execute_reply.started":"2025-07-05T03:43:14.991851Z","shell.execute_reply":"2025-07-05T03:44:32.893176Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_88/2001585076.py:24: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler() # 혼합 정밀도 학습용 스케일러\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n✅ Checkpoint loaded. Resuming from epoch 12\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 13/14 - Training:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebe749cc37e548d6b0bcc509420cf66c"}},"metadata":{}},{"name":"stdout","text":"Epoch 13 | Average Train Loss: 4.7302\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 13 - Evaluating:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cabf051621044d4a9772c66e9a892eef"}},"metadata":{}},{"name":"stdout","text":"\n--- Validation Results for Epoch 13 ---\nAverage Validation Loss: 3.5312\n\n--- Last 5 Sample Predictions ---\n🔵 Pred:  .,, a a\n a with\n with with with girl\nAG\n with with with\n with a\n with.\n\n\n AA. A sun throws holding a baseball and with and\n🟢 Label: The pitcher is wearing a red uniform shirt.\n🔵 Pred:  ,,,,, with with with\n with with with\n with\n\n with with with with with with with with with a a a a a A A in walking or sitting in for line line car\n🟢 Label: People either standing or sitting waiting in a subway.\n🔵 Pred:  ,,, a\n\n a\n D D with\n\nD D\n with with with\nANDANDAND standing WITH a..\n\n. A man and through the street. catch for the..\n🟢 Label: Young men run on the beach to train for football.\n🔵 Pred:  ,,, a,\n\n\nD with with\n\n. D A with with with\n with.. with. a\n aMAN\n A A man in standingboarding in the lake.\n🟢 Label: a man is wakeboarding in a lake\n🔵 Pred:  .,,,\n\n with with\n with with\n\n\nA's with with with\n\nA�.. a\n\nED\n A A man man player is throwing a bat bat.\n🟢 Label: A male baseball player is holding a baseball.\n---------------------------------------\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 14/14 - Training:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3cdb3818c4e40ea8e5f814c3557d4f8"}},"metadata":{}},{"name":"stdout","text":"Epoch 14 | Average Train Loss: 3.4292\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 14 - Evaluating:   0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba10e8d2463744c9b18b940ce77831bb"}},"metadata":{}},{"name":"stdout","text":"\n--- Validation Results for Epoch 14 ---\nAverage Validation Loss: 4.2926\n\n--- Last 5 Sample Predictions ---\n🔵 Pred:  .............. man.. man. man ) ) ) ) ) )\n man man ) ) A man player in the uniform uniform. a ball to a court...\n🟢 Label: A baseball player in a red jersey throwing a ball at the pitchers mound.\n🔵 Pred:  ....... man man man. ).. man. man man ) ) ) ) ) ) ) ) ) ) ) ) ) A in walking on the street.. for the train to\n🟢 Label: People are standing on a train platform waiting for the train.\n🔵 Pred:  ....... man ).. ).. man.. ). ) ) ) ) ) ) ) ) ) ) ) ) A man of people children in playing towards. the track.\n🟢 Label: A group of young men are running together on a beach.\n🔵 Pred:  ...... ) man...... man man. ). ) ) ) ). ) ) ) ) man ) ) A man inhes flying in a sense of\n🟢 Label: A man kite surfing creates a wave.\n🔵 Pred:  .............. man...... ) ) ) ) ). ). ) ) A man in a uniform background glove standing a man background. standing a ball. the park field.\n🟢 Label: A man in a gray baseball uniform and a black hat is catching a ball in a baseball mit.\n---------------------------------------\n\n✅ Checkpoint saved to checkpoints/epoch_14\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"shutil.make_archive(\"/kaggle/working/checkpoints\", 'zip', \"/kaggle/working/checkpoints\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T03:44:32.897387Z","iopub.execute_input":"2025-07-05T03:44:32.897731Z","execution_failed":"2025-07-05T03:45:06.959Z"}},"outputs":[],"execution_count":null}]}