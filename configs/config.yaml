# -- Directories --
path:
  train_csv_path: "/path/to/train.csv"
  eval_csv_path: "/path/to/eval.csv"
  image_dir: None
  save_dir: "../outputs/checkpoints"
  instruct_df: "../datasets/llava_instruct_150k.json"
  coco2014: "../datasets/train2014"
  stage1_checkpoint: "../outputs/checkpoints/ImageCaptioning-stage1/epoch9/checkpoint_epoch9.pt"
  stage2_checkpoint: "../outputs/checkpoints/ImageCaptioning-stage2/epoch20/checkpoint_epoch920pt"
  resume_from_checkpoint: null # Or "/path/to/checkpoint.pt"

# -- Datasets --
dataset:
  cocoCaption: "lmms-lab/COCO-Caption" # Only use 'val'
  flickr: "lmms-lab/flickr30k"
  textCaps: "lmms-lab/TextCaps"
  llava: "lmms-lab/LLaVA-ReCap-558K"
  vqav2: "lmms-lab/VQAv2"
  gqa: ["lmms-lab/GQA", ["train_balanced_images", "train_balanced_instructions"]] # train_balanced_instructions, train_balanced_images
  llavaNext: "lmms-lab/LLaVA-NeXT-Data"
  instruct150: "liuhaotian/LLaVA-Instruct-150K"
  aokvqa: "HuggingFaceM4/A-OKVQA"
  okvqa: "lmms-lab/OK-VQA"
  visual7w: "nimapourjafar/mm_visual7w"


# -- Model Configuration --
model:
  vision_model_name: "Salesforce/blip2-opt-2.7b"
  llm_name:  "microsoft/phi-1_5"
  num_query_tokens: 32
  lora:
    r: 16
    lora_alpha: 32
    lora_dropout: 0.05
    target_modules: ["q_proj", "k_proj", "v_proj", "dense", "fc1", "fc2"]
    bias: "none"
    task_type: "CAUSAL_LM"

# -- Training arguments --
training:
  num_epochs: 50
  batch_size: 64
  learning_rate: 1e-5
  weight_decay: 0.01
  warmup_ratio: 0.1
  tokenizer_max_length: 180

hf:
  repo_id: 'DRPARKSTREET/BLIP2_PHI1.5'
