# -- Directories --
path:
  train_csv_path: "/path/to/train.csv"
  eval_csv_path: "/path/to/eval.csv"
  image_dir: None
  save_dir: "../outputs/checkpoints"
  stage1_checkpoint: "../outputs/checkpoints/ImageCaptioning-stage1/epoch9/checkpoint_epoch9.pt"
  stage2_checkpoint: "../outputs/checkpoints/ImageCaptioning-stage2/epoch20/checkpoint_epoch920pt"
  resume_from_checkpoint: null # Or "/path/to/checkpoint.pt"

# -- Datasets --
dataset:
  cocoCaption: "lmms-lab/COCO-Caption" # Only use 'val'
  flickr: "lmms-lab/flickr30k"
  llava: "lmms-lab/LLaVA-ReCap-558K"
  vqav2: "lmms-lab/VQAv2"
  okvqa: "lmms-lab/OK-VQA"
  gqa: ["lmms-lab/GQA", ["train_balanced_images", "train_balanced_instructions"]] # train_balanced_instructions, train_balanced_images

# -- Model Configuration --
model:
  vision_model_name: "Salesforce/blip2-opt-2.7b"
  llm_name:  "microsoft/phi-1_5"
  num_query_tokens: 32
  lora:
    r: 16
    lora_alpha: 32
    lora_dropout: 0.05
    target_modules: ["q_proj", "k_proj", "v_proj", "dense", "fc1", "fc2"]
    bias: "none"
    task_type: "CAUSAL_LM"

# -- Training arguments --
training:
  num_epochs: 10
  batch_size: 4
  learning_rate: 1e-5
  weight_decay: 0.01
  warmup_ratio: 0.1
  tokenizer_max_length: 64

hf:
  repo_id: 'DRPARKSTREET/BLIP2_PHI1.5'

prompt:
  captioning: [
    "A short image caption: \n Answer :",
    "A short image description: \n Answer :",
    "Write a short description for the image \n Answer :",
    "Write a description for the photo \n Answer :",
    "Provide a description of what is presented in the photo. \n Answer :",
    "Briefly describe the content of the image. \n Answer :",
    "Can you briefly explain what you see in the image? \n Answer :",
    "Could you use a few words to describe what you perceive in the photo? \n Answer :",
    "Please provide a short depiction of the picture. \n Answer :",
    "Using language, provide a short account of the image. \n Answer :",
    "Use a few words to illustrate what is happening in the picture. \n Answer :"
  ]